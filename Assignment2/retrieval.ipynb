{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIUxomDsz7wg"
   },
   "source": [
    "# Retrieval exercise\n",
    "\n",
    "In this exercise, you will implement the query likelihood model with Jelinek-Mercer smoothing. This assignment builds on the previous assignment for creating a Pyserini index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDW16Ax3z7wk"
   },
   "source": [
    "## 1. Build the index\n",
    "Download the MS MARCO passage collection and build an index using [Pyserini](https://github.com/castorini/pyserini).\n",
    "This code is similar to PART 1 of the indexing assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZnegnVEz7wl",
    "outputId": "9a253077-f4a6-4f7f-d5fa-483039bfa0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyserini in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
      "Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.5.3)\n",
      "Requirement already satisfied: pyjnius>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.11.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.66.1)\n",
      "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.33.2)\n",
      "Requirement already satisfied: sentencepiece>=0.1.95 in /usr/local/lib/python3.10/dist-packages (from pyserini) (0.1.99)\n",
      "Requirement already satisfied: nmslib>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (2.1.1)\n",
      "Requirement already satisfied: onnxruntime>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.15.1)\n",
      "Requirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.0.0)\n",
      "Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.6.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from pyserini) (6.0.1)\n",
      "Requirement already satisfied: pybind11<2.6.2 in /usr/local/lib/python3.10/dist-packages (from nmslib>=2.1.1->pyserini) (2.6.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=2.1.1->pyserini) (5.9.5)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.5.26)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2023.3.post1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (67.7.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.17.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.3.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.6.0->pyserini) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.6.0->pyserini) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.4.0->pyserini) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.1->pyserini) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.1->pyserini) (0.1.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.2.1->pyserini) (8.1.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.8.1->pyserini) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.3.0)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
      "fatal: destination path 'anserini' already exists and is not an empty directory.\n",
      "--2023-09-19 10:53:22--  https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz\n",
      "Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 20.150.34.4\n",
      "Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|20.150.34.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1035009698 (987M) [application/octet-stream]\n",
      "Saving to: ‘data/msmarco_passage/collection.tar.gz.1’\n",
      "\n",
      "collection.tar.gz.1 100%[===================>] 987.06M  12.3MB/s    in 85s     \n",
      "\n",
      "2023-09-19 10:54:48 (11.6 MB/s) - ‘data/msmarco_passage/collection.tar.gz.1’ saved [1035009698/1035009698]\n",
      "\n",
      "collection.tsv\n",
      "Converting collection...\n",
      "Converted 0 docs, writing into file 1\n",
      "Converted 100,000 docs, writing into file 1\n",
      "Converted 200,000 docs, writing into file 1\n",
      "Converted 300,000 docs, writing into file 1\n",
      "Converted 400,000 docs, writing into file 1\n",
      "Converted 500,000 docs, writing into file 1\n",
      "Converted 600,000 docs, writing into file 1\n",
      "Converted 700,000 docs, writing into file 1\n",
      "Converted 800,000 docs, writing into file 1\n",
      "Converted 900,000 docs, writing into file 1\n",
      "Converted 1,000,000 docs, writing into file 2\n",
      "Converted 1,100,000 docs, writing into file 2\n",
      "Converted 1,200,000 docs, writing into file 2\n",
      "Converted 1,300,000 docs, writing into file 2\n",
      "Converted 1,400,000 docs, writing into file 2\n",
      "Converted 1,500,000 docs, writing into file 2\n",
      "Converted 1,600,000 docs, writing into file 2\n",
      "Converted 1,700,000 docs, writing into file 2\n",
      "Converted 1,800,000 docs, writing into file 2\n",
      "Converted 1,900,000 docs, writing into file 2\n",
      "Converted 2,000,000 docs, writing into file 3\n",
      "Converted 2,100,000 docs, writing into file 3\n",
      "Converted 2,200,000 docs, writing into file 3\n",
      "Converted 2,300,000 docs, writing into file 3\n",
      "Converted 2,400,000 docs, writing into file 3\n",
      "Converted 2,500,000 docs, writing into file 3\n",
      "Converted 2,600,000 docs, writing into file 3\n",
      "Converted 2,700,000 docs, writing into file 3\n",
      "Converted 2,800,000 docs, writing into file 3\n",
      "Converted 2,900,000 docs, writing into file 3\n",
      "Converted 3,000,000 docs, writing into file 4\n",
      "Converted 3,100,000 docs, writing into file 4\n",
      "Converted 3,200,000 docs, writing into file 4\n",
      "Converted 3,300,000 docs, writing into file 4\n",
      "Converted 3,400,000 docs, writing into file 4\n",
      "Converted 3,500,000 docs, writing into file 4\n",
      "Converted 3,600,000 docs, writing into file 4\n",
      "Converted 3,700,000 docs, writing into file 4\n",
      "Converted 3,800,000 docs, writing into file 4\n",
      "Converted 3,900,000 docs, writing into file 4\n",
      "Converted 4,000,000 docs, writing into file 5\n",
      "Converted 4,100,000 docs, writing into file 5\n",
      "Converted 4,200,000 docs, writing into file 5\n",
      "Converted 4,300,000 docs, writing into file 5\n",
      "Converted 4,400,000 docs, writing into file 5\n",
      "Converted 4,500,000 docs, writing into file 5\n",
      "Converted 4,600,000 docs, writing into file 5\n",
      "Converted 4,700,000 docs, writing into file 5\n",
      "Converted 4,800,000 docs, writing into file 5\n",
      "Converted 4,900,000 docs, writing into file 5\n",
      "Converted 5,000,000 docs, writing into file 6\n",
      "Converted 5,100,000 docs, writing into file 6\n",
      "Converted 5,200,000 docs, writing into file 6\n",
      "Converted 5,300,000 docs, writing into file 6\n",
      "Converted 5,400,000 docs, writing into file 6\n",
      "Converted 5,500,000 docs, writing into file 6\n",
      "Converted 5,600,000 docs, writing into file 6\n",
      "Converted 5,700,000 docs, writing into file 6\n",
      "Converted 5,800,000 docs, writing into file 6\n",
      "Converted 5,900,000 docs, writing into file 6\n",
      "Converted 6,000,000 docs, writing into file 7\n",
      "Converted 6,100,000 docs, writing into file 7\n",
      "Converted 6,200,000 docs, writing into file 7\n",
      "Converted 6,300,000 docs, writing into file 7\n",
      "Converted 6,400,000 docs, writing into file 7\n",
      "Converted 6,500,000 docs, writing into file 7\n",
      "Converted 6,600,000 docs, writing into file 7\n",
      "Converted 6,700,000 docs, writing into file 7\n",
      "Converted 6,800,000 docs, writing into file 7\n",
      "Converted 6,900,000 docs, writing into file 7\n",
      "Converted 7,000,000 docs, writing into file 8\n",
      "Converted 7,100,000 docs, writing into file 8\n",
      "Converted 7,200,000 docs, writing into file 8\n",
      "Converted 7,300,000 docs, writing into file 8\n",
      "Converted 7,400,000 docs, writing into file 8\n",
      "Converted 7,500,000 docs, writing into file 8\n",
      "Converted 7,600,000 docs, writing into file 8\n",
      "Converted 7,700,000 docs, writing into file 8\n",
      "Converted 7,800,000 docs, writing into file 8\n",
      "Converted 7,900,000 docs, writing into file 8\n",
      "Converted 8,000,000 docs, writing into file 9\n",
      "Converted 8,100,000 docs, writing into file 9\n",
      "Converted 8,200,000 docs, writing into file 9\n",
      "Converted 8,300,000 docs, writing into file 9\n",
      "Converted 8,400,000 docs, writing into file 9\n",
      "Converted 8,500,000 docs, writing into file 9\n",
      "Converted 8,600,000 docs, writing into file 9\n",
      "Converted 8,700,000 docs, writing into file 9\n",
      "Converted 8,800,000 docs, writing into file 9\n",
      "Done!\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2023-09-19 10:56:54,246 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2023-09-19 10:56:54,249 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2023-09-19 10:56:54,249 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2023-09-19 10:56:54,249 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/msmarco_passage/collection_jsonl\n",
      "2023-09-19 10:56:54,250 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2023-09-19 10:56:54,250 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2023-09-19 10:56:54,251 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 9\n",
      "2023-09-19 10:56:54,251 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2023-09-19 10:56:54,251 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2023-09-19 10:56:54,252 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2023-09-19 10:56:54,252 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2023-09-19 10:56:54,252 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
      "2023-09-19 10:56:54,253 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
      "2023-09-19 10:56:54,253 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2023-09-19 10:56:54,253 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2023-09-19 10:56:54,256 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2023-09-19 10:56:54,256 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2023-09-19 10:56:54,257 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2023-09-19 10:56:54,257 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
      "2023-09-19 10:56:54,258 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: indexes/lucene-index-msmarco-passage\n",
      "2023-09-19 10:56:54,264 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2023-09-19 10:56:54,282 INFO  [main] index.IndexCollection (IndexCollection.java:468) - Using DefaultEnglishAnalyzer\n",
      "2023-09-19 10:56:54,283 INFO  [main] index.IndexCollection (IndexCollection.java:469) - Stemmer: porter\n",
      "2023-09-19 10:56:54,283 INFO  [main] index.IndexCollection (IndexCollection.java:470) - Keep stopwords? false\n",
      "2023-09-19 10:56:54,284 INFO  [main] index.IndexCollection (IndexCollection.java:471) - Stopwords file: null\n",
      "2023-09-19 10:56:54,551 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 9 threads initialized.\n",
      "2023-09-19 10:56:54,552 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/msmarco_passage/collection_jsonl\n",
      "2023-09-19 10:56:54,557 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 9 files found\n",
      "2023-09-19 10:56:54,558 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2023-09-19 10:57:54,584 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 450,000 documents indexed\n",
      "2023-09-19 10:58:54,586 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 1,170,000 documents indexed\n",
      "2023-09-19 10:59:54,588 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 1,900,000 documents indexed\n",
      "2023-09-19 11:00:54,589 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 2,630,000 documents indexed\n",
      "2023-09-19 11:01:54,590 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 3,310,000 documents indexed\n",
      "2023-09-19 11:02:54,591 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 4,060,000 documents indexed\n",
      "2023-09-19 11:03:54,601 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 4,770,000 documents indexed\n",
      "2023-09-19 11:04:54,605 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 5,390,000 documents indexed\n",
      "2023-09-19 11:05:54,606 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 5,960,000 documents indexed\n",
      "2023-09-19 11:06:54,608 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 6,630,000 documents indexed\n",
      "2023-09-19 11:07:54,609 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 7,240,000 documents indexed\n",
      "2023-09-19 11:08:07,175 DEBUG [pool-2-thread-3] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs08.json: 841823 docs added.\n",
      "2023-09-19 11:08:54,614 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 11.11% of files completed, 7,821,823 documents indexed\n",
      "2023-09-19 11:09:54,615 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 11.11% of files completed, 8,391,823 documents indexed\n",
      "2023-09-19 11:10:13,866 DEBUG [pool-2-thread-9] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs07.json: 1000000 docs added.\n",
      "2023-09-19 11:10:24,281 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs00.json: 1000000 docs added.\n",
      "2023-09-19 11:10:26,374 DEBUG [pool-2-thread-4] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs01.json: 1000000 docs added.\n",
      "2023-09-19 11:10:28,703 DEBUG [pool-2-thread-2] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs02.json: 1000000 docs added.\n",
      "2023-09-19 11:10:29,811 DEBUG [pool-2-thread-8] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs03.json: 1000000 docs added.\n",
      "2023-09-19 11:10:30,773 DEBUG [pool-2-thread-6] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs04.json: 1000000 docs added.\n",
      "2023-09-19 11:10:33,444 DEBUG [pool-2-thread-5] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs05.json: 1000000 docs added.\n",
      "2023-09-19 11:10:36,790 DEBUG [pool-2-thread-7] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs06.json: 1000000 docs added.\n",
      "2023-09-19 11:12:01,306 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 8,841,823 documents indexed\n",
      "2023-09-19 11:12:01,307 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2023-09-19 11:12:01,307 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:        8,841,823\n",
      "2023-09-19 11:12:01,307 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2023-09-19 11:12:01,308 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2023-09-19 11:12:01,308 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2023-09-19 11:12:01,308 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2023-09-19 11:12:01,324 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 8,841,823 documents indexed in 00:15:07\n"
     ]
    }
   ],
   "source": [
    "!pip install pyserini\n",
    "!pip install faiss-cpu\n",
    "\n",
    "!git clone https://github.com/castorini/anserini.git --recurse-submodules\n",
    "\n",
    "!wget https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz -P data/msmarco_passage/\n",
    "!tar xvfz data/msmarco_passage/collection.tar.gz -C data/msmarco_passage\n",
    "\n",
    "!cd anserini && python tools/scripts/msmarco/convert_collection_to_jsonl.py \\\n",
    " --collection-path ../data/msmarco_passage/collection.tsv --output-folder ../data/msmarco_passage/collection_jsonl\n",
    "\n",
    "!rm data/msmarco_passage/*.tsv\n",
    "!rm -rf sample_data\n",
    "\n",
    "!python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator -threads 9 \\\n",
    "-input data/msmarco_passage/collection_jsonl -index indexes/lucene-index-msmarco-passage -storePositions -storeDocvectors -storeRaw\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y7fHsrjz7wn"
   },
   "source": [
    "## 2. Download and read the query file\n",
    "You will rank MSMARCO passages for this set of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0PFg9CG94mr",
    "outputId": "2fa60302-dfbf-458a-80aa-7a4280a512e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-19 11:12:01--  http://gem.cs.ru.nl/IR-Course/queries.txt\n",
      "Resolving gem.cs.ru.nl (gem.cs.ru.nl)... 131.174.31.31\n",
      "Connecting to gem.cs.ru.nl (gem.cs.ru.nl)|131.174.31.31|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2275 (2.2K) [text/plain]\n",
      "Saving to: ‘queries.txt.1’\n",
      "\n",
      "queries.txt.1       100%[===================>]   2.22K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-09-19 11:12:02 (291 MB/s) - ‘queries.txt.1’ saved [2275/2275]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://gem.cs.ru.nl/IR-Course/queries.txt\n",
    "\n",
    "queries = dict()\n",
    "with open(\"queries.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        cols = line.split(\"\\t\")\n",
    "        queries[cols[0].strip()] = cols[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqQ9dLKGz7wo"
   },
   "source": [
    "## 3. Implement the retrieval model\n",
    "You will implement language model with Jelinek-Mercer (JM) smoothing:\n",
    "$$score(q,d) = \\sum_{t \\in q} log ((1-\\lambda) \\frac{c(t, d)}{|d|} + \\lambda \\frac{c (t, C)}{|C|}),$$\n",
    "where $c (t, d)$ and $c (t, C)$ represent frequency of a term in a document and collection, respectively.\n",
    "\n",
    "**Notes about your implementation:**\n",
    "- Skip a term if it does not exist in the whole collection. This avoids $log(0)$.\n",
    "- Make sure to use the right form of a query (analyzed vs. not analyzed)\n",
    "- Use natural logarithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aTvGGMez7wo"
   },
   "source": [
    "### 3.1. Obtain collection length\n",
    "In this code, the global variable `len_C` denotes collection length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wm_dvrdGz7wp"
   },
   "outputs": [],
   "source": [
    "from pyserini.index.lucene import IndexReader\n",
    "\n",
    "global len_C\n",
    "\n",
    "# =======Your code=======\n",
    "index_reader = IndexReader('indexes/lucene-index-msmarco-passage')\n",
    "len_C = index_reader.stats()['total_terms']\n",
    "# =======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLw-PtK7z7wp"
   },
   "source": [
    "Run this to test your code. If everything is correct, you should not get errors here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VPp4gWOcz7wq"
   },
   "outputs": [],
   "source": [
    "assert len_C == 352316036"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsAxcO6Sz7wq"
   },
   "source": [
    "### 3.2. Obtain document length\n",
    "\n",
    "Here you need compute the length of document (as it is stored in the index).\n",
    "\n",
    "*Hint: You first need to get the document vector from your Pyserini index. Consult [Pyseniri documentation](https://github.com/castorini/pyserini/blob/master/docs/usage-indexreader.md) to find the right function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LgNvo5ySz7wq"
   },
   "outputs": [],
   "source": [
    "def len_doc(d):\n",
    "    # =======Your code=======\n",
    "    doc_vec = index_reader.get_document_vector(d)\n",
    "    len_d = sum(doc_vec.values())\n",
    "    # =======================\n",
    "    return len_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pyTXkaStz7wr"
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "assert len_doc(\"2674124\") == 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mo_ZvZZz7wr"
   },
   "source": [
    "### 3.3. Obtain collection frequency of a term\n",
    "\n",
    "Obtain number of times a term appers in the whole collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "e5DjkOL4z7ws"
   },
   "outputs": [],
   "source": [
    "def coll_freq(t):\n",
    "    # =======Your code=======\n",
    "    df, cf = index_reader.get_term_counts(t)\n",
    "    # =======================\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jORCqhcYz7ws"
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "assert coll_freq(\"record\") == 226439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IskuP3d0z7wt"
   },
   "source": [
    "### 3.4. Obtain term frequency\n",
    "\n",
    "Obtain number of times a term appears in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_NACqgxZz7wt"
   },
   "outputs": [],
   "source": [
    "def term_freq(t, d):\n",
    "    # =======Your code=======\n",
    "    doc_vec = index_reader.get_document_vector(d)\n",
    "    tf = doc_vec[t] if t in doc_vec else 0\n",
    "    # =======================\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "r_zvVALfz7wt"
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "assert term_freq(\"record\", \"2674124\") == 2\n",
    "assert term_freq(\"presence\", \"2674124\") == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPNZ3Q-Zz7wu"
   },
   "source": [
    "### 3.5. Compute JM-smoothed probability for a single term\n",
    "\n",
    "Here, you need to implement the following formula:\n",
    "\n",
    "$$P_{JM}(t,d) = (1-\\lambda) \\frac{c(t, d)}{|d|} + \\lambda \\frac{c (t, C)}{|C|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "oxP7NRtnz7wu"
   },
   "outputs": [],
   "source": [
    "def prob_t_Md(t, d, lambd):\n",
    "    # =======Your code=======\n",
    "    p_t_Md = ((1-lambd)*(term_freq(t,d)/len_doc(d)))+(lambd*(coll_freq(t)/len_C))\n",
    "    # =======================\n",
    "    return p_t_Md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Z05E7Tniz7wu"
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "assert prob_t_Md(\"record\", \"2674124\", 0.1) == 0.05812878768549357\n",
    "assert prob_t_Md(\"darcig\", \"2674124\", 0.1) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayHxi_bFz7wv"
   },
   "source": [
    "### 3.6. Compute JM-smoothed probability for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "s6gTHha7z7wv"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def score_doc(q, d, lambd):\n",
    "    # =======Your code=======\n",
    "    p_q_Md = 0\n",
    "    for term in q:\n",
    "      # Skip term analysis:\n",
    "      df, cf = index_reader.get_term_counts(term, analyzer=None)\n",
    "      if cf == 0 :\n",
    "        continue\n",
    "      p_q_Md += math.log(prob_t_Md(term,d,lambd))\n",
    "    # =======================\n",
    "    return p_q_Md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JTTwVuCQz7ww"
   },
   "outputs": [],
   "source": [
    "q1 = index_reader.analyze(\"are naturalization records public\")\n",
    "q2 = index_reader.analyze(\"kemeet land\")\n",
    "doc = \"2674124\"\n",
    "assert score_doc(q1, doc, 0.1) == -9.227787624348021\n",
    "assert score_doc(q2, doc, 0.1) == -10.254756777887694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8n2OLTUz7wx"
   },
   "source": [
    "## 4. Rank documents for the given queries\n",
    "Ranking is done in two steps:\n",
    "1. First pass retrieval: Use a fast ranker (i.e., Pyserini LuceneSearcher) ro rank all documents for a given query.\n",
    "2. Second pass retrieval: Re-rank top-100 documents from the 1st pass retrieval using your retrieval model. This is to make the ranking process efficient.\n",
    "\n",
    "**Notes:**\n",
    "- You need to change the default values of LuceneSearcher functions to obtain top-100 documents\n",
    "- Set the value of lambda to 0.1\n",
    "- Store your final ranking results in the `results` variable. Every item in the `results` list is a list containing queryID, documentID, and score. This is an example how the content of results should look like:\n",
    "\n",
    "`[['23849', '4348282', -10.65],\n",
    " ['23849', '7119957', -12.63],\n",
    " ['23849', '', -17.687729001682484],\n",
    " ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "oF74VkRg94m8"
   },
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "lambd = 0.1\n",
    "results = []\n",
    "searcher = LuceneSearcher(\"indexes/lucene-index-msmarco-passage\")\n",
    "for qid, q in queries.items():\n",
    "    # =======Your code=======\n",
    "    hits = searcher.search(q,100)\n",
    "    analyzed_q = index_reader.analyze(q)\n",
    "    for hit in hits:\n",
    "      score = score_doc(analyzed_q,hit.docid,lambd)\n",
    "      results.append([qid,hit.docid,score])\n",
    "    # ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tLfZfYyz7wx",
    "outputId": "a9f679f5-d13b-435d-dde5-a7ba2783f502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-160109.875\n"
     ]
    }
   ],
   "source": [
    "# Test your code\n",
    "print(round(sum([item[2] for item in results]), 3))\n",
    "assert round(sum([item[2] for item in results]), 3) == -160109.875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zUuHqbCz7wy"
   },
   "source": [
    "Write your results into a file.\n",
    "Submit this file together with the completed notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iex2B2Bz7wy",
    "outputId": "6001e826-dab8-42f2-ccd9-d5c28c71c71e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246907"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicates\n",
    "check = set()\n",
    "for res in results:\n",
    "    if ((res[0], res[1])) in check:\n",
    "        raise Exception(\"Error: Duplicate query-doc is found\", res[0], res[1])\n",
    "    check.add((res[0], res[1]))\n",
    "\n",
    "# write results in a file\n",
    "output_str = \"\\n\".join([l[0] + \"\\tQ0\\t\" + l[1] + \"\\t0\\t\" + str(l[2]) + \"\\tlm_jm\" for l in results])\n",
    "open(\"lm_jm.run\", \"w\").write(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr17hR57z7wy"
   },
   "source": [
    "## Handing in\n",
    "Submit the result file (ranked documents), the filled-in notebook, and the pdf version of your notebook:\n",
    "\n",
    "- The result file should be named STUDENTNUMBER_FIRSTNAME_LASTNAME_lm_jm.run\n",
    "- The notebook should be named STUDENTNUMBER_FIRSTNAME_LASTNAME_retrieval.ipynb\n",
    "- The pdf version of your notebook should be named STUDENTNUMBER_FIRSTNAME_LASTNAME_retrieval.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "bDNNeoF9z7wz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
